{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('D:/research/jokesclustering/') # ваш путь до корня проекта\n",
    "from vector_clustering.data.manager import get_jokes_as_dataframe, read_lines_from_file\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Мужчину трудно задеть за живое, но уж если зад...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В нашем кемпинге строго запрещено людям разног...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А как хорошо у девушек начинается: любимый: ми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Одна белка случайно попробовала пиво и поняла,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ОБЪЯВЛЕНИЕ На время мирового финансового кризи...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           joke_text\n",
       "0  Мужчину трудно задеть за живое, но уж если зад...\n",
       "1  В нашем кемпинге строго запрещено людям разног...\n",
       "2  А как хорошо у девушек начинается: любимый: ми...\n",
       "3  Одна белка случайно попробовала пиво и поняла,...\n",
       "4  ОБЪЯВЛЕНИЕ На время мирового финансового кризи..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_frame = get_jokes_as_dataframe()\n",
    "jokes_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции предобработки текста\n",
    "лемматизаторы, чистка стоп-слов, любой необходимый препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_simple_preprocess(text, min_len=3, max_len=1000000):\n",
    "    ## приводит к нижнему регистру, удаляет знаки препинания, оставляет слова между (min_length, max_length)\n",
    "    return ' '.join(simple_preprocess(text, min_len=min_len, max_len=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnmorph_lemmatizer(text):\n",
    "    ## альтернатива для pymorphy2: приводит слова к нормальной форме.\n",
    "    return ' '.join([x.normal_form for x in predictor.predict(text.split())])\n",
    "\n",
    "def pymorphy_lemmatizer(text):\n",
    "    return ' '.join(analyzer.parse(x)[0].normal_form for x in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([x.split('\\n')[0] for x in read_lines_from_file('ru_stop_words.txt')])\n",
    "\n",
    "def stop_words_remove(text):\n",
    "    ## удаление из текста стоп-слов на основании списка стоп-слов\n",
    "    return ' '.join([x for x in text.lower().split() if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### сделать в качестве словаря только Tfidf ?\n",
    "## если сделать Tfidf, то можно убрать из словаря самые редкие слова и самые частые\n",
    "### доделать !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяет порядок вызова функций\n",
    "# rnnmorph_lemmatizer слишком долгий\n",
    "cleaners = [gensim_simple_preprocess, stop_words_remove, pymorphy_lemmatizer]\n",
    "\n",
    "def apply_to_text(t):\n",
    "    for func in cleaners:\n",
    "        t = func(t)\n",
    "    return t\n",
    "\n",
    "def apply_func_to_texts(texts, func):\n",
    "    return [func(t) for t in texts]\n",
    "\n",
    "def apply_to_texts(ts):\n",
    "    return [apply_to_text(t) for t in ts]\n",
    "\n",
    "def apply_to_texts_func_by_func(texts):\n",
    "    for func in cleaners:\n",
    "        texts = apply_func_to_texts(texts, func)\n",
    "        print('Applied {}'.format(func))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied <function gensim_simple_preprocess at 0x0000019B31CEB2F0>\n",
      "Applied <function stop_words_remove at 0x0000019B29AE1048>\n",
      "Applied <function pymorphy_lemmatizer at 0x0000019B31CEB598>\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocessed_collection = apply_to_texts_func_by_func(jokes_frame.joke_text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение lda модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_collection = [x.split() for x in preprocessed_collection]\n",
    "dictionary = corpora.Dictionary(tokenized_collection) # словарь вида: dictionary[word] : numeric_index\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_collection] # каждый документ теперь выглядит так:\n",
    "# [(12, 3), (25, 2), ... ] - то есть в шутке словоа с индексом 12 встретилось 3 раза, слово с индексом 25 - 2 раза и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, \n",
    "                      num_topics=number_topics, \n",
    "                      id2word=dictionary, \n",
    "                      dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0, words: 0.042*\"мясо\" + 0.035*\"помогать\" + 0.035*\"командировка\" + 0.030*\"мор\" + 0.026*\"старик\"\n",
      "\n",
      "Topic: 1, words: 0.206*\"давать\" + 0.078*\"брать\" + 0.047*\"кот\" + 0.042*\"попасть\" + 0.034*\"кухня\"\n",
      "\n",
      "Topic: 2, words: 0.048*\"прямо\" + 0.047*\"кризис\" + 0.044*\"десять\" + 0.040*\"общий\" + 0.039*\"вызывать\"\n",
      "\n",
      "Topic: 3, words: 0.066*\"долго\" + 0.060*\"новое\" + 0.055*\"сэр\" + 0.042*\"изя\" + 0.036*\"похожий\"\n",
      "\n",
      "Topic: 4, words: 0.093*\"магазин\" + 0.080*\"телефон\" + 0.057*\"никогда\" + 0.054*\"учительница\" + 0.033*\"продавец\"\n",
      "\n",
      "Topic: 5, words: 0.213*\"другой\" + 0.072*\"всегда\" + 0.048*\"рассказать\" + 0.040*\"сторона\" + 0.031*\"хотеться\"\n",
      "\n",
      "Topic: 6, words: 0.151*\"потом\" + 0.070*\"курить\" + 0.060*\"завтра\" + 0.058*\"выпить\" + 0.044*\"зарплата\"\n",
      "\n",
      "Topic: 7, words: 0.107*\"мальчик\" + 0.083*\"девочка\" + 0.062*\"вода\" + 0.057*\"забыть\" + 0.054*\"дедушка\"\n",
      "\n",
      "Topic: 8, words: 0.148*\"девушка\" + 0.135*\"день\" + 0.117*\"или\" + 0.062*\"парень\" + 0.054*\"здесь\"\n",
      "\n",
      "Topic: 9, words: 0.200*\"хотеть\" + 0.123*\"делать\" + 0.110*\"надо\" + 0.088*\"ничто\" + 0.050*\"быть\"\n",
      "\n",
      "Topic: 10, words: 0.151*\"очень\" + 0.144*\"любить\" + 0.073*\"много\" + 0.046*\"уйти\" + 0.029*\"сюда\"\n",
      "\n",
      "Topic: 11, words: 0.113*\"ведь\" + 0.112*\"нибыть\" + 0.056*\"четыре\" + 0.049*\"тёща\" + 0.024*\"представить\"\n",
      "\n",
      "Topic: 12, words: 0.107*\"вопрос\" + 0.091*\"отвечать\" + 0.087*\"таки\" + 0.069*\"рабин\" + 0.055*\"нужный\"\n",
      "\n",
      "Topic: 13, words: 0.119*\"дорогой\" + 0.117*\"деньга\" + 0.107*\"купить\" + 0.060*\"маленький\" + 0.041*\"быть\"\n",
      "\n",
      "Topic: 14, words: 0.333*\"спрашивать\" + 0.060*\"бабушка\" + 0.036*\"чтоб\" + 0.036*\"старый\" + 0.024*\"гулять\"\n",
      "\n",
      "Topic: 15, words: 0.117*\"думать\" + 0.109*\"теперь\" + 0.107*\"ребёнок\" + 0.065*\"отец\" + 0.060*\"быть\"\n",
      "\n",
      "Topic: 16, words: 0.100*\"посмотреть\" + 0.085*\"утро\" + 0.080*\"час\" + 0.067*\"остаться\" + 0.038*\"уходить\"\n",
      "\n",
      "Topic: 17, words: 0.155*\"дверь\" + 0.048*\"покупать\" + 0.044*\"миллиард\" + 0.036*\"посадить\" + 0.033*\"внимание\"\n",
      "\n",
      "Topic: 18, words: 0.130*\"под\" + 0.048*\"бутылка\" + 0.046*\"что\" + 0.038*\"открыть\" + 0.033*\"упасть\"\n",
      "\n",
      "Topic: 19, words: 0.105*\"российский\" + 0.101*\"ответ\" + 0.049*\"палец\" + 0.039*\"солдат\" + 0.037*\"американский\"\n",
      "\n",
      "Topic: 20, words: 0.123*\"спросить\" + 0.092*\"квартира\" + 0.063*\"чёрный\" + 0.056*\"верить\" + 0.039*\"дерево\"\n",
      "\n",
      "Topic: 21, words: 0.091*\"блондинка\" + 0.063*\"вернуться\" + 0.033*\"детство\" + 0.030*\"английский\" + 0.024*\"два\"\n",
      "\n",
      "Topic: 22, words: 0.130*\"через\" + 0.096*\"пожалуйста\" + 0.093*\"должный\" + 0.062*\"уметь\" + 0.062*\"быть\"\n",
      "\n",
      "Topic: 23, words: 0.103*\"опять\" + 0.068*\"настенёк\" + 0.054*\"дядя\" + 0.045*\"принести\" + 0.030*\"белый\"\n",
      "\n",
      "Topic: 24, words: 0.070*\"туда\" + 0.051*\"цена\" + 0.048*\"правительство\" + 0.035*\"разный\" + 0.024*\"соседка\"\n",
      "\n",
      "Topic: 25, words: 0.137*\"раз\" + 0.104*\"тоже\" + 0.089*\"сделать\" + 0.086*\"один\" + 0.068*\"два\"\n",
      "\n",
      "Topic: 26, words: 0.123*\"пить\" + 0.117*\"взять\" + 0.057*\"народ\" + 0.055*\"помочь\" + 0.034*\"показать\"\n",
      "\n",
      "Topic: 27, words: 0.081*\"послать\" + 0.033*\"хлеб\" + 0.030*\"порядок\" + 0.030*\"сигарета\" + 0.028*\"грустный\"\n",
      "\n",
      "Topic: 28, words: 0.134*\"конечно\" + 0.081*\"иван\" + 0.055*\"ряд\" + 0.054*\"стоять\" + 0.041*\"ночью\"\n",
      "\n",
      "Topic: 29, words: 0.086*\"звонить\" + 0.068*\"милый\" + 0.052*\"добрый\" + 0.047*\"следующий\" + 0.043*\"комната\"\n",
      "\n",
      "Topic: 30, words: 0.086*\"улица\" + 0.069*\"пьяный\" + 0.061*\"смочь\" + 0.059*\"месяц\" + 0.045*\"право\"\n",
      "\n",
      "Topic: 31, words: 0.044*\"попросить\" + 0.036*\"ум\" + 0.033*\"слава\" + 0.033*\"идиот\" + 0.032*\"праздник\"\n",
      "\n",
      "Topic: 32, words: 0.149*\"подходить\" + 0.065*\"умный\" + 0.039*\"предлагать\" + 0.030*\"для\" + 0.028*\"крым\"\n",
      "\n",
      "Topic: 33, words: 0.124*\"сейчас\" + 0.116*\"дело\" + 0.086*\"стоить\" + 0.080*\"владимир\" + 0.077*\"вдруг\"\n",
      "\n",
      "Topic: 34, words: 0.111*\"школа\" + 0.057*\"жопа\" + 0.053*\"считать\" + 0.038*\"худой\" + 0.032*\"чуть\"\n",
      "\n",
      "Topic: 35, words: 0.206*\"новый\" + 0.056*\"снять\" + 0.045*\"прийтись\" + 0.022*\"здоровье\" + 0.019*\"штаны\"\n",
      "\n",
      "Topic: 36, words: 0.213*\"три\" + 0.083*\"глаз\" + 0.080*\"привет\" + 0.036*\"два\" + 0.028*\"компьютер\"\n",
      "\n",
      "Topic: 37, words: 0.093*\"заниматься\" + 0.048*\"называться\" + 0.042*\"хватать\" + 0.036*\"еда\" + 0.035*\"счастливый\"\n",
      "\n",
      "Topic: 38, words: 0.270*\"там\" + 0.150*\"работа\" + 0.042*\"любовь\" + 0.025*\"учиться\" + 0.024*\"газета\"\n",
      "\n",
      "Topic: 39, words: 0.117*\"заходить\" + 0.045*\"съесть\" + 0.030*\"нечего\" + 0.028*\"отношение\" + 0.026*\"стыдно\"\n",
      "\n",
      "Topic: 40, words: 0.098*\"писать\" + 0.052*\"назвать\" + 0.052*\"рассказывать\" + 0.045*\"буква\" + 0.044*\"никакой\"\n",
      "\n",
      "Topic: 41, words: 0.068*\"жениться\" + 0.057*\"фамилия\" + 0.056*\"сын\" + 0.043*\"армия\" + 0.029*\"устать\"\n",
      "\n",
      "Topic: 42, words: 0.212*\"знать\" + 0.050*\"дочь\" + 0.042*\"дорога\" + 0.036*\"номер\" + 0.036*\"дура\"\n",
      "\n",
      "Topic: 43, words: 0.193*\"тут\" + 0.087*\"спать\" + 0.061*\"знаете\" + 0.059*\"собака\" + 0.041*\"земля\"\n",
      "\n",
      "Topic: 44, words: 0.102*\"пять\" + 0.043*\"яйцо\" + 0.036*\"продавать\" + 0.032*\"сила\" + 0.030*\"вырасти\"\n",
      "\n",
      "Topic: 45, words: 0.082*\"просить\" + 0.039*\"обращаться\" + 0.037*\"суд\" + 0.036*\"пациент\" + 0.033*\"деревня\"\n",
      "\n",
      "Topic: 46, words: 0.132*\"без\" + 0.100*\"знаешь\" + 0.086*\"значит\" + 0.041*\"если\" + 0.033*\"бить\"\n",
      "\n",
      "Topic: 47, words: 0.110*\"вчера\" + 0.102*\"хорошо\" + 0.070*\"найти\" + 0.061*\"решить\" + 0.056*\"быть\"\n",
      "\n",
      "Topic: 48, words: 0.063*\"красный\" + 0.047*\"вечер\" + 0.046*\"болеть\" + 0.041*\"попробовать\" + 0.025*\"живой\"\n",
      "\n",
      "Topic: 49, words: 0.222*\"идти\" + 0.085*\"врач\" + 0.071*\"прийти\" + 0.057*\"молодая\" + 0.041*\"говорить\"\n",
      "\n",
      "Topic: 50, words: 0.099*\"выходить\" + 0.085*\"звать\" + 0.069*\"спасибо\" + 0.058*\"наверное\" + 0.051*\"представлять\"\n",
      "\n",
      "Topic: 51, words: 0.256*\"хороший\" + 0.064*\"плохой\" + 0.027*\"автомобиль\" + 0.023*\"старое\" + 0.022*\"быть\"\n",
      "\n",
      "Topic: 52, words: 0.101*\"ехать\" + 0.093*\"красивый\" + 0.044*\"власть\" + 0.042*\"недавно\" + 0.034*\"украинский\"\n",
      "\n",
      "Topic: 53, words: 0.076*\"узнать\" + 0.048*\"пройти\" + 0.043*\"судья\" + 0.034*\"высокий\" + 0.033*\"козёл\"\n",
      "\n",
      "Topic: 54, words: 0.251*\"жена\" + 0.177*\"муж\" + 0.102*\"женщина\" + 0.071*\"мужчина\" + 0.060*\"машина\"\n",
      "\n",
      "Topic: 55, words: 0.056*\"дочка\" + 0.054*\"водитель\" + 0.037*\"поднять\" + 0.028*\"задний\" + 0.027*\"карман\"\n",
      "\n",
      "Topic: 56, words: 0.061*\"скоро\" + 0.037*\"зато\" + 0.034*\"корова\" + 0.026*\"прошлый\" + 0.023*\"быть\"\n",
      "\n",
      "Topic: 57, words: 0.323*\"вот\" + 0.122*\"тогда\" + 0.046*\"дед\" + 0.031*\"говорить\" + 0.030*\"господин\"\n",
      "\n",
      "Topic: 58, words: 0.082*\"ответить\" + 0.064*\"правильно\" + 0.051*\"абрам\" + 0.048*\"снимать\" + 0.025*\"учёный\"\n",
      "\n",
      "Topic: 59, words: 0.088*\"читать\" + 0.058*\"ладный\" + 0.047*\"женский\" + 0.044*\"вечером\" + 0.034*\"туалет\"\n",
      "\n",
      "Topic: 60, words: 0.100*\"место\" + 0.068*\"мир\" + 0.045*\"самый\" + 0.036*\"леса\" + 0.031*\"аптека\"\n",
      "\n",
      "Topic: 61, words: 0.239*\"сидеть\" + 0.050*\"ага\" + 0.034*\"чиновник\" + 0.028*\"анна\" + 0.020*\"сволочь\"\n",
      "\n",
      "Topic: 62, words: 0.089*\"правда\" + 0.043*\"жаловаться\" + 0.036*\"золотой\" + 0.035*\"сходить\" + 0.035*\"мент\"\n",
      "\n",
      "Topic: 63, words: 0.091*\"голос\" + 0.088*\"вообще\" + 0.068*\"начинать\" + 0.065*\"точно\" + 0.055*\"вид\"\n",
      "\n",
      "Topic: 64, words: 0.169*\"друг\" + 0.060*\"утром\" + 0.059*\"окно\" + 0.047*\"увидеть\" + 0.040*\"бывать\"\n",
      "\n",
      "Topic: 65, words: 0.238*\"дать\" + 0.079*\"нужно\" + 0.077*\"говорят\" + 0.035*\"именно\" + 0.023*\"поднимать\"\n",
      "\n",
      "Topic: 66, words: 0.130*\"ночь\" + 0.101*\"москва\" + 0.060*\"город\" + 0.037*\"сесть\" + 0.035*\"запретить\"\n",
      "\n",
      "Topic: 67, words: 0.106*\"ходить\" + 0.097*\"сынок\" + 0.043*\"садиться\" + 0.038*\"немного\" + 0.025*\"сердце\"\n",
      "\n",
      "Topic: 68, words: 0.076*\"слышать\" + 0.075*\"президент\" + 0.069*\"встречаться\" + 0.051*\"лежать\" + 0.050*\"нельзя\"\n",
      "\n",
      "Topic: 69, words: 0.122*\"домой\" + 0.071*\"новость\" + 0.051*\"возвращаться\" + 0.048*\"член\" + 0.046*\"война\"\n",
      "\n",
      "Topic: 70, words: 0.061*\"пара\" + 0.059*\"товарищ\" + 0.056*\"поставить\" + 0.024*\"подождать\" + 0.023*\"центр\"\n",
      "\n",
      "Topic: 71, words: 0.176*\"доктор\" + 0.055*\"вася\" + 0.038*\"показывать\" + 0.038*\"одесса\" + 0.030*\"медведь\"\n",
      "\n",
      "Topic: 72, words: 0.235*\"видеть\" + 0.097*\"водка\" + 0.034*\"равно\" + 0.031*\"петька\" + 0.027*\"молодец\"\n",
      "\n",
      "Topic: 73, words: 0.067*\"получить\" + 0.065*\"бояться\" + 0.036*\"хрен\" + 0.036*\"шесть\" + 0.030*\"потерять\"\n",
      "\n",
      "Topic: 74, words: 0.220*\"дом\" + 0.044*\"телевизор\" + 0.035*\"понятно\" + 0.034*\"объявление\" + 0.031*\"приём\"\n",
      "\n",
      "Topic: 75, words: 0.166*\"слово\" + 0.042*\"радио\" + 0.038*\"двор\" + 0.022*\"дворник\" + 0.022*\"будто\"\n",
      "\n",
      "Topic: 76, words: 0.194*\"мама\" + 0.159*\"папа\" + 0.051*\"выйти\" + 0.047*\"сын\" + 0.039*\"подарить\"\n",
      "\n",
      "Topic: 77, words: 0.143*\"рука\" + 0.094*\"голова\" + 0.073*\"нога\" + 0.072*\"помнить\" + 0.038*\"хотя\"\n",
      "\n",
      "Topic: 78, words: 0.143*\"жить\" + 0.070*\"совсем\" + 0.067*\"ждать\" + 0.060*\"лицо\" + 0.040*\"убить\"\n",
      "\n",
      "Topic: 79, words: 0.166*\"путин\" + 0.074*\"про\" + 0.069*\"медведев\" + 0.066*\"год\" + 0.041*\"президент\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 80, words: 0.117*\"никто\" + 0.068*\"урок\" + 0.040*\"провести\" + 0.040*\"янукович\" + 0.034*\"некоторый\"\n",
      "\n",
      "Topic: 81, words: 0.095*\"страна\" + 0.076*\"неделя\" + 0.067*\"двое\" + 0.045*\"язык\" + 0.042*\"начальник\"\n",
      "\n",
      "Topic: 82, words: 0.085*\"хоть\" + 0.047*\"бежать\" + 0.045*\"положить\" + 0.041*\"просыпаться\" + 0.038*\"счёт\"\n",
      "\n",
      "Topic: 83, words: 0.108*\"даже\" + 0.094*\"понимать\" + 0.072*\"сосед\" + 0.070*\"украина\" + 0.046*\"бросить\"\n",
      "\n",
      "Topic: 84, words: 0.085*\"свет\" + 0.073*\"конец\" + 0.046*\"оказаться\" + 0.045*\"закон\" + 0.038*\"март\"\n",
      "\n",
      "Topic: 85, words: 0.052*\"слушай\" + 0.046*\"директор\" + 0.045*\"половина\" + 0.044*\"смысл\" + 0.036*\"платить\"\n",
      "\n",
      "Topic: 86, words: 0.161*\"русский\" + 0.084*\"пока\" + 0.083*\"здравствовать\" + 0.052*\"случай\" + 0.035*\"продать\"\n",
      "\n",
      "Topic: 87, words: 0.124*\"при\" + 0.094*\"иметь\" + 0.057*\"часы\" + 0.035*\"более\" + 0.029*\"действительно\"\n",
      "\n",
      "Topic: 88, words: 0.091*\"два\" + 0.078*\"разговаривать\" + 0.068*\"подруга\" + 0.049*\"люся\" + 0.034*\"она\"\n",
      "\n",
      "Topic: 89, words: 0.129*\"работать\" + 0.117*\"пойти\" + 0.063*\"тысяча\" + 0.034*\"поехать\" + 0.031*\"россия\"\n",
      "\n",
      "Topic: 90, words: 0.093*\"еврей\" + 0.064*\"блин\" + 0.061*\"любимый\" + 0.043*\"гаишник\" + 0.026*\"зелёный\"\n",
      "\n",
      "Topic: 91, words: 0.229*\"смотреть\" + 0.065*\"доллар\" + 0.050*\"приехать\" + 0.043*\"пусть\" + 0.030*\"свадьба\"\n",
      "\n",
      "Topic: 92, words: 0.071*\"пора\" + 0.037*\"тип\" + 0.033*\"обычно\" + 0.031*\"мимо\" + 0.028*\"странно\"\n",
      "\n",
      "Topic: 93, words: 0.147*\"рубль\" + 0.064*\"извинить\" + 0.034*\"познакомиться\" + 0.030*\"сидоров\" + 0.028*\"здоровый\"\n",
      "\n",
      "Topic: 94, words: 0.158*\"вовочка\" + 0.070*\"начать\" + 0.066*\"минута\" + 0.065*\"звонок\" + 0.051*\"миллион\"\n",
      "\n",
      "Topic: 95, words: 0.198*\"сегодня\" + 0.084*\"перед\" + 0.062*\"главный\" + 0.040*\"выбор\" + 0.018*\"зоопарк\"\n",
      "\n",
      "Topic: 96, words: 0.118*\"алло\" + 0.038*\"иванушка\" + 0.032*\"немец\" + 0.024*\"нос\" + 0.024*\"завести\"\n",
      "\n",
      "Topic: 97, words: 0.131*\"бог\" + 0.085*\"играть\" + 0.058*\"полный\" + 0.025*\"свидание\" + 0.023*\"бросать\"\n",
      "\n",
      "Topic: 98, words: 0.258*\"мужик\" + 0.066*\"секс\" + 0.047*\"последний\" + 0.043*\"баба\" + 0.023*\"два\"\n",
      "\n",
      "Topic: 99, words: 0.160*\"приходить\" + 0.087*\"понять\" + 0.071*\"сразу\" + 0.046*\"нравиться\" + 0.046*\"мочь\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(number_topics):\n",
    "    print('Topic: {}, words: {}'.format(idx, lda.print_topic(idx, 5)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Присвоение тем шуткам\n",
    "на выходе получим дата фрейм из одной колонки, эта колонка - самая вероятная тема для данной шутки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_relevant_topics = [sorted(lda.get_document_topics(bow), key=lambda x: x[1], reverse=True)[0][0] for bow in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'topic_id' : most_relevant_topics}).to_csv('lda_100_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_frame['topic_id'] = most_relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_text</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Мужчину трудно задеть за живое, но уж если зад...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В нашем кемпинге строго запрещено людям разног...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А как хорошо у девушек начинается: любимый: ми...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Одна белка случайно попробовала пиво и поняла,...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ОБЪЯВЛЕНИЕ На время мирового финансового кризи...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           joke_text  topic_id\n",
       "0  Мужчину трудно задеть за живое, но уж если зад...        65\n",
       "1  В нашем кемпинге строго запрещено людям разног...        66\n",
       "2  А как хорошо у девушек начинается: любимый: ми...        96\n",
       "3  Одна белка случайно попробовала пиво и поняла,...        48\n",
       "4  ОБЪЯВЛЕНИЕ На время мирового финансового кризи...         2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для отображения результатов\n",
    "теперь тема описывается словами. Чтобы понять, что это за тема и оценить ее качество нужно посмотреть на топ слов и на шутки данной темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descirbe_topic(topic_id, top_words=10, top_jokes=10, need_print=True, writing_object=None):\n",
    "    most_relevant_words = ', '.join(x[0] for x in lda.show_topic(topic_id, topn=top_words))\n",
    "    sample_jokes = '\\n'.join(jokes_frame[jokes_frame.topic_id == topic_id].sample(top_jokes).joke_text.values)\n",
    "    template = '=====\\nTopic {}\\nTop words: {}\\nSample jokes:\\n {}\\n=====\\n\\n'.format(topic_id, most_relevant_words, sample_jokes)\n",
    "    if need_print:\n",
    "        print(template)\n",
    "    if writing_object:\n",
    "        writing_object.write(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_100_topics_description.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(number_topics):\n",
    "        descirbe_topic(topic_id=i, need_print=False, writing_object=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
