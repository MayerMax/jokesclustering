{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('D:/research/jokesclustering/') # ваш путь до корня проекта\n",
    "from vector_clustering.data.manager import get_jokes_as_dataframe, read_lines_from_file\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Мужчину трудно задеть за живое, но уж если зад...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В нашем кемпинге строго запрещено людям разног...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А как хорошо у девушек начинается: любимый: ми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Одна белка случайно попробовала пиво и поняла,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ОБЪЯВЛЕНИЕ На время мирового финансового кризи...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           joke_text\n",
       "0  Мужчину трудно задеть за живое, но уж если зад...\n",
       "1  В нашем кемпинге строго запрещено людям разног...\n",
       "2  А как хорошо у девушек начинается: любимый: ми...\n",
       "3  Одна белка случайно попробовала пиво и поняла,...\n",
       "4  ОБЪЯВЛЕНИЕ На время мирового финансового кризи..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_frame = get_jokes_as_dataframe()\n",
    "jokes_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции предобработки текста\n",
    "лемматизаторы, чистка стоп-слов, любой необходимый препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_simple_preprocess(text, min_len=3, max_len=1000000):\n",
    "    ## приводит к нижнему регистру, удаляет знаки препинания, оставляет слова между (min_length, max_length)\n",
    "    return ' '.join(simple_preprocess(text, min_len=min_len, max_len=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnmorph_lemmatizer(text):\n",
    "    ## альтернатива для pymorphy2: приводит слова к нормальной форме.\n",
    "    return ' '.join([x.normal_form for x in predictor.predict(text.split())])\n",
    "\n",
    "def pymorphy_lemmatizer(text):\n",
    "    return ' '.join(analyzer.parse(x)[0].normal_form for x in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([x.split('\\n')[0] for x in read_lines_from_file('ru_stop_words.txt')])\n",
    "\n",
    "def stop_words_remove(text):\n",
    "    ## удаление из текста стоп-слов на основании списка стоп-слов\n",
    "    return ' '.join([x for x in text.lower().split() if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### сделать в качестве словаря только Tfidf ?\n",
    "## если сделать Tfidf, то можно убрать из словаря самые редкие слова и самые частые\n",
    "### доделать !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяет порядок вызова функций\n",
    "# rnnmorph_lemmatizer слишком долгий\n",
    "cleaners = [gensim_simple_preprocess, stop_words_remove, pymorphy_lemmatizer]\n",
    "\n",
    "def apply_to_text(t):\n",
    "    for func in cleaners:\n",
    "        t = func(t)\n",
    "    return t\n",
    "\n",
    "def apply_func_to_texts(texts, func):\n",
    "    return [func(t) for t in texts]\n",
    "\n",
    "def apply_to_texts(ts):\n",
    "    return [apply_to_text(t) for t in ts]\n",
    "\n",
    "def apply_to_texts_func_by_func(texts):\n",
    "    for func in cleaners:\n",
    "        texts = apply_func_to_texts(texts, func)\n",
    "        print('Applied {}'.format(func))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied <function gensim_simple_preprocess at 0x0000019B31CEB2F0>\n",
      "Applied <function stop_words_remove at 0x0000019B29AE1048>\n",
      "Applied <function pymorphy_lemmatizer at 0x0000019B31CEB598>\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocessed_collection = apply_to_texts_func_by_func(jokes_frame.joke_text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение lda модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_collection = [x.split() for x in preprocessed_collection]\n",
    "dictionary = corpora.Dictionary(tokenized_collection) # словарь вида: dictionary[word] : numeric_index\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_collection] # каждый документ теперь выглядит так:\n",
    "# [(12, 3), (25, 2), ... ] - то есть в шутке слово с индексом 12 встретилось 3 раза, слово с индексом 25 - 2 раза и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, \n",
    "                      num_topics=number_topics, \n",
    "                      id2word=dictionary, \n",
    "                      dtype=np.float64,\n",
    "                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0, words: 0.125*\"даже\" + 0.043*\"буква\" + 0.028*\"немец\" + 0.027*\"шеф\" + 0.023*\"ругаться\"\n",
      "\n",
      "Topic: 1, words: 0.235*\"или\" + 0.167*\"конечно\" + 0.031*\"разница\" + 0.022*\"собирать\" + 0.019*\"надеяться\"\n",
      "\n",
      "Topic: 2, words: 0.114*\"рабин\" + 0.066*\"добрый\" + 0.053*\"бывать\" + 0.031*\"официант\" + 0.027*\"камера\"\n",
      "\n",
      "Topic: 3, words: 0.104*\"найти\" + 0.074*\"час\" + 0.071*\"ехать\" + 0.035*\"прямо\" + 0.034*\"прийтись\"\n",
      "\n",
      "Topic: 4, words: 0.172*\"теперь\" + 0.106*\"нибыть\" + 0.062*\"неделя\" + 0.059*\"собака\" + 0.042*\"быть\"\n",
      "\n",
      "Topic: 5, words: 0.109*\"подумать\" + 0.070*\"главный\" + 0.068*\"выйти\" + 0.048*\"гаишник\" + 0.046*\"оказаться\"\n",
      "\n",
      "Topic: 6, words: 0.190*\"делать\" + 0.135*\"ничто\" + 0.047*\"плохой\" + 0.046*\"уметь\" + 0.040*\"попасть\"\n",
      "\n",
      "Topic: 7, words: 0.239*\"жена\" + 0.155*\"дом\" + 0.065*\"написать\" + 0.048*\"дочь\" + 0.030*\"садиться\"\n",
      "\n",
      "Topic: 8, words: 0.209*\"тогда\" + 0.069*\"хоть\" + 0.049*\"жопа\" + 0.031*\"общий\" + 0.029*\"брат\"\n",
      "\n",
      "Topic: 9, words: 0.094*\"посмотреть\" + 0.089*\"должный\" + 0.088*\"быть\" + 0.076*\"дед\" + 0.049*\"стол\"\n",
      "\n",
      "Topic: 10, words: 0.067*\"снять\" + 0.051*\"появиться\" + 0.043*\"гражданин\" + 0.033*\"связь\" + 0.031*\"предложить\"\n",
      "\n",
      "Topic: 11, words: 0.095*\"российский\" + 0.066*\"дедушка\" + 0.052*\"хотя\" + 0.049*\"принять\" + 0.031*\"встретиться\"\n",
      "\n",
      "Topic: 12, words: 0.355*\"спрашивать\" + 0.090*\"ночь\" + 0.037*\"плакать\" + 0.036*\"комната\" + 0.023*\"результат\"\n",
      "\n",
      "Topic: 13, words: 0.128*\"сейчас\" + 0.117*\"один\" + 0.056*\"просить\" + 0.056*\"быть\" + 0.055*\"минута\"\n",
      "\n",
      "Topic: 14, words: 0.230*\"тут\" + 0.063*\"умереть\" + 0.036*\"кофе\" + 0.032*\"золотой\" + 0.026*\"смерть\"\n",
      "\n",
      "Topic: 15, words: 0.099*\"никто\" + 0.057*\"первое\" + 0.049*\"половина\" + 0.047*\"смысл\" + 0.039*\"интересно\"\n",
      "\n",
      "Topic: 16, words: 0.250*\"хотеть\" + 0.106*\"очень\" + 0.101*\"любить\" + 0.048*\"звать\" + 0.041*\"доллар\"\n",
      "\n",
      "Topic: 17, words: 0.191*\"деньга\" + 0.102*\"понимать\" + 0.038*\"кризис\" + 0.031*\"трус\" + 0.030*\"аптека\"\n",
      "\n",
      "Topic: 18, words: 0.090*\"настенёк\" + 0.037*\"мясо\" + 0.029*\"счастие\" + 0.028*\"угол\" + 0.023*\"детский\"\n",
      "\n",
      "Topic: 19, words: 0.328*\"год\" + 0.123*\"новый\" + 0.054*\"заниматься\" + 0.038*\"быть\" + 0.035*\"назад\"\n",
      "\n",
      "Topic: 20, words: 0.125*\"взять\" + 0.066*\"нужно\" + 0.052*\"лицо\" + 0.049*\"выпить\" + 0.046*\"уйти\"\n",
      "\n",
      "Topic: 21, words: 0.144*\"стать\" + 0.084*\"опять\" + 0.082*\"водка\" + 0.050*\"смочь\" + 0.036*\"гость\"\n",
      "\n",
      "Topic: 22, words: 0.089*\"россия\" + 0.077*\"ждать\" + 0.049*\"проходить\" + 0.043*\"мимо\" + 0.033*\"банка\"\n",
      "\n",
      "Topic: 23, words: 0.045*\"мент\" + 0.037*\"здоровый\" + 0.033*\"чаять\" + 0.028*\"странно\" + 0.024*\"размер\"\n",
      "\n",
      "Topic: 24, words: 0.067*\"никогда\" + 0.062*\"любовь\" + 0.057*\"представлять\" + 0.044*\"она\" + 0.036*\"называть\"\n",
      "\n",
      "Topic: 25, words: 0.112*\"заходить\" + 0.041*\"суд\" + 0.035*\"поздравлять\" + 0.033*\"праздник\" + 0.033*\"уехать\"\n",
      "\n",
      "Topic: 26, words: 0.060*\"право\" + 0.057*\"зарплата\" + 0.053*\"несколько\" + 0.038*\"василий\" + 0.029*\"нести\"\n",
      "\n",
      "Topic: 27, words: 0.377*\"вот\" + 0.072*\"звонить\" + 0.051*\"здравствовать\" + 0.029*\"искать\" + 0.021*\"компьютер\"\n",
      "\n",
      "Topic: 28, words: 0.061*\"поставить\" + 0.060*\"рассказывать\" + 0.049*\"положить\" + 0.041*\"продать\" + 0.030*\"зелёный\"\n",
      "\n",
      "Topic: 29, words: 0.172*\"машина\" + 0.052*\"показать\" + 0.050*\"водитель\" + 0.034*\"получиться\" + 0.032*\"батюшка\"\n",
      "\n",
      "Topic: 30, words: 0.131*\"потом\" + 0.081*\"магазин\" + 0.077*\"вообще\" + 0.056*\"двое\" + 0.050*\"дурак\"\n",
      "\n",
      "Topic: 31, words: 0.078*\"подарить\" + 0.071*\"любимый\" + 0.038*\"деревня\" + 0.026*\"мор\" + 0.025*\"центр\"\n",
      "\n",
      "Topic: 32, words: 0.238*\"раз\" + 0.059*\"два\" + 0.059*\"месяц\" + 0.041*\"вечер\" + 0.033*\"кажется\"\n",
      "\n",
      "Topic: 33, words: 0.072*\"дядя\" + 0.061*\"постель\" + 0.060*\"дура\" + 0.049*\"замуж\" + 0.040*\"голов\"\n",
      "\n",
      "Topic: 34, words: 0.263*\"идти\" + 0.060*\"знаете\" + 0.047*\"наконец\" + 0.044*\"плохо\" + 0.029*\"правительство\"\n",
      "\n",
      "Topic: 35, words: 0.099*\"читать\" + 0.054*\"чёрный\" + 0.038*\"туалет\" + 0.033*\"чуть\" + 0.032*\"корова\"\n",
      "\n",
      "Topic: 36, words: 0.086*\"правда\" + 0.083*\"проблема\" + 0.053*\"земля\" + 0.036*\"пациент\" + 0.034*\"вызывать\"\n",
      "\n",
      "Topic: 37, words: 0.260*\"мужик\" + 0.075*\"владимир\" + 0.067*\"при\" + 0.043*\"баба\" + 0.040*\"блин\"\n",
      "\n",
      "Topic: 38, words: 0.078*\"здесь\" + 0.075*\"секс\" + 0.070*\"сынок\" + 0.060*\"бабушка\" + 0.054*\"мать\"\n",
      "\n",
      "Topic: 39, words: 0.106*\"вопрос\" + 0.098*\"через\" + 0.057*\"помнить\" + 0.050*\"лежать\" + 0.047*\"ответить\"\n",
      "\n",
      "Topic: 40, words: 0.125*\"вдруг\" + 0.057*\"далёкий\" + 0.049*\"слушать\" + 0.038*\"держать\" + 0.027*\"бегать\"\n",
      "\n",
      "Topic: 41, words: 0.068*\"нравиться\" + 0.053*\"директор\" + 0.053*\"вечером\" + 0.039*\"изя\" + 0.038*\"ужас\"\n",
      "\n",
      "Topic: 42, words: 0.176*\"мама\" + 0.147*\"день\" + 0.056*\"выходить\" + 0.052*\"улица\" + 0.039*\"каждый\"\n",
      "\n",
      "Topic: 43, words: 0.183*\"видеть\" + 0.143*\"три\" + 0.046*\"четыре\" + 0.042*\"красный\" + 0.022*\"газета\"\n",
      "\n",
      "Topic: 44, words: 0.088*\"пьяный\" + 0.084*\"играть\" + 0.058*\"считать\" + 0.046*\"попробовать\" + 0.034*\"сидоров\"\n",
      "\n",
      "Topic: 45, words: 0.205*\"девушка\" + 0.115*\"пить\" + 0.086*\"парень\" + 0.043*\"извинить\" + 0.037*\"правильно\"\n",
      "\n",
      "Topic: 46, words: 0.070*\"утром\" + 0.055*\"товарищ\" + 0.053*\"следующий\" + 0.044*\"сша\" + 0.043*\"вместе\"\n",
      "\n",
      "Topic: 47, words: 0.176*\"рука\" + 0.073*\"окно\" + 0.035*\"левый\" + 0.027*\"дмитрий\" + 0.025*\"правый\"\n",
      "\n",
      "Topic: 48, words: 0.189*\"другой\" + 0.080*\"два\" + 0.065*\"всегда\" + 0.022*\"похожий\" + 0.022*\"есть\"\n",
      "\n",
      "Topic: 49, words: 0.133*\"школа\" + 0.055*\"убить\" + 0.043*\"учиться\" + 0.039*\"советский\" + 0.038*\"сила\"\n",
      "\n",
      "Topic: 50, words: 0.146*\"рубль\" + 0.065*\"узнать\" + 0.040*\"армия\" + 0.031*\"быстрый\" + 0.026*\"помогать\"\n",
      "\n",
      "Topic: 51, words: 0.200*\"муж\" + 0.130*\"жена\" + 0.113*\"дать\" + 0.073*\"президент\" + 0.035*\"подруга\"\n",
      "\n",
      "Topic: 52, words: 0.140*\"вчера\" + 0.071*\"спросить\" + 0.061*\"начать\" + 0.054*\"свет\" + 0.037*\"пусть\"\n",
      "\n",
      "Topic: 53, words: 0.130*\"знаешь\" + 0.065*\"вид\" + 0.061*\"пара\" + 0.036*\"провести\" + 0.031*\"врать\"\n",
      "\n",
      "Topic: 54, words: 0.065*\"вася\" + 0.059*\"скоро\" + 0.045*\"быть\" + 0.036*\"штирлиц\" + 0.028*\"сайт\"\n",
      "\n",
      "Topic: 55, words: 0.079*\"приехать\" + 0.041*\"знакомый\" + 0.034*\"лошадь\" + 0.033*\"причина\" + 0.025*\"герой\"\n",
      "\n",
      "Topic: 56, words: 0.165*\"дорогой\" + 0.074*\"много\" + 0.059*\"курить\" + 0.045*\"господин\" + 0.035*\"тётя\"\n",
      "\n",
      "Topic: 57, words: 0.088*\"сосед\" + 0.054*\"что\" + 0.049*\"сторона\" + 0.033*\"лететь\" + 0.029*\"рыба\"\n",
      "\n",
      "Topic: 58, words: 0.112*\"бог\" + 0.067*\"стоять\" + 0.054*\"фамилия\" + 0.031*\"быть\" + 0.030*\"анна\"\n",
      "\n",
      "Topic: 59, words: 0.073*\"бояться\" + 0.047*\"леса\" + 0.038*\"дерево\" + 0.031*\"действительно\" + 0.030*\"танк\"\n",
      "\n",
      "Topic: 60, words: 0.114*\"место\" + 0.066*\"нормальный\" + 0.040*\"господь\" + 0.029*\"принимать\" + 0.028*\"гей\"\n",
      "\n",
      "Topic: 61, words: 0.205*\"смотреть\" + 0.104*\"ведь\" + 0.038*\"интернет\" + 0.037*\"часы\" + 0.033*\"класс\"\n",
      "\n",
      "Topic: 62, words: 0.109*\"решить\" + 0.077*\"помочь\" + 0.046*\"болеть\" + 0.036*\"трубка\" + 0.027*\"петров\"\n",
      "\n",
      "Topic: 63, words: 0.092*\"привет\" + 0.065*\"ряд\" + 0.053*\"разговор\" + 0.041*\"самолёт\" + 0.031*\"чувствовать\"\n",
      "\n",
      "Topic: 64, words: 0.106*\"под\" + 0.096*\"врач\" + 0.090*\"дверь\" + 0.061*\"нога\" + 0.058*\"тысяча\"\n",
      "\n",
      "Topic: 65, words: 0.084*\"пиво\" + 0.068*\"полный\" + 0.039*\"сара\" + 0.033*\"старое\" + 0.025*\"соседка\"\n",
      "\n",
      "Topic: 66, words: 0.236*\"женщина\" + 0.166*\"мужчина\" + 0.048*\"член\" + 0.023*\"если\" + 0.020*\"мочь\"\n",
      "\n",
      "Topic: 67, words: 0.223*\"там\" + 0.124*\"работа\" + 0.068*\"спать\" + 0.065*\"ходить\" + 0.034*\"случиться\"\n",
      "\n",
      "Topic: 68, words: 0.239*\"давать\" + 0.091*\"брать\" + 0.041*\"женский\" + 0.033*\"попросить\" + 0.033*\"снимать\"\n",
      "\n",
      "Topic: 69, words: 0.100*\"таки\" + 0.084*\"пожалуйста\" + 0.068*\"телефон\" + 0.064*\"украина\" + 0.057*\"блондинка\"\n",
      "\n",
      "Topic: 70, words: 0.070*\"жениться\" + 0.058*\"ага\" + 0.049*\"телевизор\" + 0.047*\"начальник\" + 0.045*\"называться\"\n",
      "\n",
      "Topic: 71, words: 0.148*\"сын\" + 0.111*\"отец\" + 0.050*\"учительница\" + 0.043*\"номер\" + 0.042*\"вон\"\n",
      "\n",
      "Topic: 72, words: 0.198*\"папа\" + 0.068*\"утро\" + 0.053*\"мир\" + 0.045*\"ладный\" + 0.041*\"возвращаться\"\n",
      "\n",
      "Topic: 73, words: 0.124*\"голова\" + 0.091*\"ответ\" + 0.056*\"новое\" + 0.055*\"депутат\" + 0.043*\"хватать\"\n",
      "\n",
      "Topic: 74, words: 0.083*\"голос\" + 0.064*\"звонок\" + 0.054*\"вода\" + 0.046*\"урок\" + 0.045*\"бутылка\"\n",
      "\n",
      "Topic: 75, words: 0.134*\"слово\" + 0.097*\"писать\" + 0.031*\"идиот\" + 0.025*\"конь\" + 0.022*\"получаться\"\n",
      "\n",
      "Topic: 76, words: 0.092*\"про\" + 0.088*\"алло\" + 0.070*\"говорят\" + 0.052*\"получить\" + 0.048*\"рассказать\"\n",
      "\n",
      "Topic: 77, words: 0.115*\"прийти\" + 0.056*\"бросить\" + 0.054*\"собираться\" + 0.032*\"зуб\" + 0.027*\"целый\"\n",
      "\n",
      "Topic: 78, words: 0.210*\"путин\" + 0.087*\"медведев\" + 0.073*\"пока\" + 0.035*\"больной\" + 0.033*\"кабинет\"\n",
      "\n",
      "Topic: 79, words: 0.066*\"послать\" + 0.057*\"открывать\" + 0.050*\"старый\" + 0.048*\"министр\" + 0.037*\"приезжать\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 80, words: 0.168*\"работать\" + 0.095*\"москва\" + 0.056*\"город\" + 0.046*\"верить\" + 0.046*\"сэр\"\n",
      "\n",
      "Topic: 81, words: 0.153*\"надо\" + 0.108*\"хорошо\" + 0.096*\"жить\" + 0.065*\"маленький\" + 0.049*\"иметь\"\n",
      "\n",
      "Topic: 82, words: 0.167*\"приходить\" + 0.083*\"значит\" + 0.074*\"еврей\" + 0.044*\"семья\" + 0.036*\"если\"\n",
      "\n",
      "Topic: 83, words: 0.110*\"понять\" + 0.099*\"иван\" + 0.050*\"война\" + 0.033*\"почти\" + 0.029*\"грудь\"\n",
      "\n",
      "Topic: 84, words: 0.188*\"знать\" + 0.120*\"доктор\" + 0.056*\"молодая\" + 0.048*\"последний\" + 0.028*\"фильм\"\n",
      "\n",
      "Topic: 85, words: 0.166*\"ребёнок\" + 0.070*\"сразу\" + 0.041*\"слушай\" + 0.035*\"дочка\" + 0.034*\"родитель\"\n",
      "\n",
      "Topic: 86, words: 0.111*\"стоить\" + 0.087*\"встречаться\" + 0.068*\"новость\" + 0.056*\"конец\" + 0.048*\"давно\"\n",
      "\n",
      "Topic: 87, words: 0.064*\"вернуться\" + 0.042*\"отдать\" + 0.032*\"лишь\" + 0.027*\"привести\" + 0.025*\"метр\"\n",
      "\n",
      "Topic: 88, words: 0.084*\"кричать\" + 0.068*\"квартира\" + 0.061*\"кот\" + 0.057*\"миллион\" + 0.054*\"люся\"\n",
      "\n",
      "Topic: 89, words: 0.116*\"отвечать\" + 0.064*\"нельзя\" + 0.041*\"можно\" + 0.038*\"абрам\" + 0.037*\"яйцо\"\n",
      "\n",
      "Topic: 90, words: 0.113*\"русский\" + 0.077*\"слышать\" + 0.065*\"разговаривать\" + 0.061*\"девочка\" + 0.053*\"милый\"\n",
      "\n",
      "Topic: 91, words: 0.171*\"вовочка\" + 0.071*\"народ\" + 0.058*\"завтра\" + 0.054*\"анекдот\" + 0.054*\"тёща\"\n",
      "\n",
      "Topic: 92, words: 0.073*\"забыть\" + 0.057*\"приятель\" + 0.042*\"объявление\" + 0.039*\"приём\" + 0.023*\"клуб\"\n",
      "\n",
      "Topic: 93, words: 0.141*\"дело\" + 0.063*\"красивый\" + 0.050*\"наверное\" + 0.045*\"туда\" + 0.031*\"сёма\"\n",
      "\n",
      "Topic: 94, words: 0.102*\"глаз\" + 0.086*\"перед\" + 0.037*\"ухо\" + 0.032*\"всякий\" + 0.031*\"простой\"\n",
      "\n",
      "Topic: 95, words: 0.110*\"мальчик\" + 0.097*\"пять\" + 0.078*\"совсем\" + 0.073*\"остаться\" + 0.047*\"имя\"\n",
      "\n",
      "Topic: 96, words: 0.239*\"сидеть\" + 0.124*\"домой\" + 0.029*\"поймать\" + 0.027*\"кровать\" + 0.023*\"тюрьма\"\n",
      "\n",
      "Topic: 97, words: 0.184*\"купить\" + 0.063*\"студент\" + 0.045*\"орать\" + 0.033*\"экзамен\" + 0.027*\"футбол\"\n",
      "\n",
      "Topic: 98, words: 0.143*\"друг\" + 0.129*\"сделать\" + 0.092*\"подходить\" + 0.031*\"уходить\" + 0.031*\"столько\"\n",
      "\n",
      "Topic: 99, words: 0.141*\"думать\" + 0.123*\"тоже\" + 0.089*\"пойти\" + 0.047*\"нужный\" + 0.034*\"быть\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(number_topics):\n",
    "    print('Topic: {}, words: {}'.format(idx, lda.print_topic(idx, 5)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Присвоение тем шуткам\n",
    "на выходе получим дата фрейм из одной колонки, эта колонка - самая вероятная тема для данной шутки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_relevant_topics = [sorted(lda.get_document_topics(bow), key=lambda x: x[1], reverse=True)[0][0] for bow in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'topic_id' : most_relevant_topics}).to_csv('lda_100_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_frame['topic_id'] = most_relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_text</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Мужчину трудно задеть за живое, но уж если зад...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В нашем кемпинге строго запрещено людям разног...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А как хорошо у девушек начинается: любимый: ми...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Одна белка случайно попробовала пиво и поняла,...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ОБЪЯВЛЕНИЕ На время мирового финансового кризи...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           joke_text  topic_id\n",
       "0  Мужчину трудно задеть за живое, но уж если зад...        43\n",
       "1  В нашем кемпинге строго запрещено людям разног...        46\n",
       "2  А как хорошо у девушек начинается: любимый: ми...        37\n",
       "3  Одна белка случайно попробовала пиво и поняла,...        48\n",
       "4  ОБЪЯВЛЕНИЕ На время мирового финансового кризи...        17"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для отображения результатов\n",
    "теперь тема описывается словами. Чтобы понять, что это за тема и оценить ее качество нужно посмотреть на топ слов и на шутки данной темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descirbe_topic(topic_id, top_words=10, top_jokes=10, need_print=True, writing_object=None):\n",
    "    most_relevant_words = ', '.join(x[0] for x in lda.show_topic(topic_id, topn=top_words))\n",
    "    sample_jokes = '\\n'.join(jokes_frame[jokes_frame.topic_id == topic_id].sample(top_jokes).joke_text.values)\n",
    "    template = '=====\\nTopic {}\\nTop words: {}\\nSample jokes:\\n {}\\n=====\\n\\n'.format(topic_id, most_relevant_words, sample_jokes)\n",
    "    if need_print:\n",
    "        print(template)\n",
    "    if writing_object:\n",
    "        writing_object.write(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_100_topics_description.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(number_topics):\n",
    "        descirbe_topic(topic_id=i, need_print=False, writing_object=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save(fname='lda_100_topics.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
